{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakhterov/ML_algorithms_from_scratch/blob/master/machine_translation_using_bahdanau_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-9HBe6We5LC"
      },
      "source": [
        "# 0. Description\n",
        "\n",
        "We're going to build an NN model to translate from Russian to English. This notebook is committed to the implementation of the encoder-decoder network with Bahdanau attention (Additive Attention) mechanism.\n",
        "\n",
        "We will use the following terms:\n",
        "- source language - the language from which the model translates\n",
        "- target language - the language to which the model translates\n",
        "- token = word\n",
        "\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hijest/englishrussian-dictionary-for-machine-translate/\n",
        "\n",
        "References:\n",
        "- https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb\n",
        "- https://www.youtube.com/watch?v=vI2Y3I-JI2Q\n",
        "- https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b\n",
        "- https://blog.floydhub.com/attention-mechanism/#bahdanau-att-step2\n",
        "- https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrOij2Lsj3yA",
        "outputId": "2ea1bd7d-22ef-47ba-d168-2ee16dddc5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IEg-hGLJfDv"
      },
      "source": [
        "# 1. Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_RSzJxTxSmLY"
      },
      "outputs": [],
      "source": [
        "UNKNOWN_TOKEN = '[UNK]' # Out of vocabulary token\n",
        "START_TOKEN = '[START]' # The token that denotes the beginning of the target language phrase\n",
        "END_TOKEN = '[END]' # The token that denotes the end of the target language phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OwlrS_c8K6pU"
      },
      "outputs": [],
      "source": [
        "class Vectorization:\n",
        "  '''\n",
        "    Vectorization text class.\n",
        "    Main goals:\n",
        "     - make a vocabulary\n",
        "     - convert the list of strings to the list of integer tokens\n",
        "     - convert the list of integer tokens to the list of strings\n",
        "  '''\n",
        "\n",
        "  def __init__(self,\n",
        "               max_tokens,\n",
        "               max_length=None,\n",
        "               unknown_token=UNKNOWN_TOKEN,\n",
        "               start_token=START_TOKEN,\n",
        "               end_token=END_TOKEN\n",
        "               ):\n",
        "    '''\n",
        "      :param max_tokens: length of the vocabulary\n",
        "      :param max_length: max length of the phrases\n",
        "      :param unknown_token: out of vocabulary token\n",
        "      :param start_token: token that denotes the beginning of the phrase\n",
        "      :param end_token: token that denotes the end of the phrase\n",
        "    '''\n",
        "    self.max_tokens = max_tokens\n",
        "    self.max_length = max_length\n",
        "    self.unknown_token = unknown_token\n",
        "    self.start_token = start_token\n",
        "    self.end_token=end_token\n",
        "    # add to the vocabulary:\n",
        "    #  (1) padding token (we're going to pad using 0, so padding token index is 0)\n",
        "    #  (2) out of vocabulary token\n",
        "    #  (3) start token\n",
        "    #  (4) end token\n",
        "    self.vocabulary = ['', self.unknown_token, self.start_token, self.end_token]\n",
        "\n",
        "  def __preprocessing(self, input: str) -> str:\n",
        "    '''\n",
        "      Preprocess of the string (convert to lowcase and remove punctuation).\n",
        "      ex.: I'm going! -> i m going\n",
        "      :param input - input string\n",
        "      :return preprocessed string\n",
        "    '''\n",
        "    output = ''.join(map(lambda ch: ch if ch not in punctuation else ' ', input.lower())).strip()\n",
        "    return output\n",
        "\n",
        "  def token_to_text(self, tokens: List) -> str:\n",
        "    '''\n",
        "      Convert the list of the integer tokens to the string\n",
        "      :param tokens: list of the integer tokens\n",
        "      :return string contains words that correspond to the integer tokens\n",
        "    '''\n",
        "    words = [self.vocabulary[token] for token in tokens]\n",
        "    return \" \".join(words)\n",
        "\n",
        "  def fit(self, X: List):\n",
        "    '''\n",
        "      Make the vocabulary and calculate the max length of the phrase\n",
        "      :param X: corpus - list of the strings\n",
        "      :return the instance of the current class\n",
        "    '''\n",
        "    lens = []\n",
        "    for x in X:\n",
        "      # Make preprocessing and get the list of the words.\n",
        "      # Ex. I'm going! -> ['i', 'm', 'going']\n",
        "      words = self.__preprocessing(x).split()\n",
        "\n",
        "      # Collect phrases lengths\n",
        "      lens.append(len(words))\n",
        "\n",
        "      # Make the vocabulary\n",
        "      for word in words:\n",
        "        token = word.strip()\n",
        "        # Add the word to the vocabulary if it usn't \"full\"\n",
        "        if token not in self.vocabulary and self.max_tokens is not None and len(self.vocabulary)<self.max_tokens:\n",
        "          self.vocabulary.append(token)\n",
        "\n",
        "    # Calculate the max length of the phrases if it isn't set in the __init__\n",
        "    # max_length = Average length + two standard devations\n",
        "    lens = np.array(lens)\n",
        "    if self.max_length is None:\n",
        "      self.max_length = int(np.mean(lens) + 2 * np.std(lens))\n",
        "    return self\n",
        "\n",
        "  def predict(self,\n",
        "              X: List[str],\n",
        "              is_padding=True,\n",
        "              is_add_start_token=False,\n",
        "              is_add_end_token=False\n",
        "              ) -> List[List]:\n",
        "    '''\n",
        "      :param X - corpus - list of the strings\n",
        "      :param is_padding - whether to pad the list of tokens to the max. length with 0s\n",
        "      :param is_add_start_token - whether to add the start_token to the list of tokens\n",
        "      :param is_add_end_token - whether to add the end_token to the list of tokens\n",
        "      :return list of the lists of tokens\n",
        "    '''\n",
        "    output = []\n",
        "    self.max_length += int(is_add_start_token) + int(is_add_end_token)\n",
        "\n",
        "    for x in X:\n",
        "      # If nedded add the index of the start_token to the beginning of the list of tokens\n",
        "      vector = [self.vocabulary.index(self.start_token)] if is_add_start_token else []\n",
        "\n",
        "      # Make preprocessing and get the list of the words.\n",
        "      words = self.__preprocessing(x).split()\n",
        "\n",
        "      # If the current word is in the vocabulary add its index to the list else add the index of the unknown_token\n",
        "      for word in words:\n",
        "        token = word.strip()\n",
        "        vector.append(self.vocabulary.index(token) if token in self.vocabulary else self.vocabulary.index(self.unknown_token))\n",
        "\n",
        "      # Truncate the vector to the max. length\n",
        "      vector = vector[:self.max_length-1]\n",
        "\n",
        "      # If needed add the index of the end_token\n",
        "      if is_add_end_token:\n",
        "        vector.append(self.vocabulary.index(self.end_token))\n",
        "\n",
        "      output.append(vector)\n",
        "\n",
        "    # If needed pad the vector to the max. length with 0s\n",
        "    return pad_sequences(output,\n",
        "                         maxlen=self.max_length,\n",
        "                         padding='post',\n",
        "                         truncating='post') if is_padding else output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "VvaCDzcFuIe1"
      },
      "outputs": [],
      "source": [
        "# Read from N to M samples\n",
        "N = 199_000\n",
        "M = 200_000\n",
        "\n",
        "input_phrases, output_phrases = [], []\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Data/rus.txt') as f:\n",
        "  for line in f.readlines()[N:M]:\n",
        "    eng, rus = line.split('CC-BY')[0].strip().split('\\t')\n",
        "    input_phrases.append(rus)\n",
        "    output_phrases.append(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "d-LhZGvyvrkS"
      },
      "outputs": [],
      "source": [
        "input_vocab = 2000 # size of the source language vocaulary\n",
        "output_vocab = 2000 # size of the target language vocaulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "chd-W_mXVgqE"
      },
      "outputs": [],
      "source": [
        "# Make vectorization of the source language phrases\n",
        "encoder_vec = Vectorization(max_tokens=input_vocab)\n",
        "encoder_vec.fit(input_phrases)\n",
        "X_encoder = encoder_vec.predict(input_phrases, is_padding=False)\n",
        "\n",
        "# Make vectorization of the target language phrases\n",
        "decoder_vec = Vectorization(max_tokens=output_vocab)\n",
        "decoder_vec.fit(output_phrases)\n",
        "# For the reason of the sequence model training we need decoder input contains the start_token and\n",
        "# the decoder output which is without the start_token\n",
        "X_decoder = decoder_vec.predict(output_phrases, is_add_start_token=True, is_add_end_token=True)\n",
        "Y_decoder = decoder_vec.predict(output_phrases, is_add_end_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHSeTSRGQhmS",
        "outputId": "8e2a4d35-594f-44d9-83d7-c56008c12389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 100\n",
            "======= Encoder =======\n",
            "Input phrase: Том пошёл к гадалке.\n",
            "Vector: [8, 197, 148, 220]\n",
            "Max. length: 7\n",
            "======= Decoder =======\n",
            "Input phrase: Tom went to a fortune teller.\n",
            "Vector: [  2   4 168  20  18 183 184   3   0]\n",
            "Output phrase: Tom went to a fortune teller.\n",
            "Vector: [  4 168  20  18 183 184   3   0   0   0]\n",
            "Max. length: 10\n",
            "==============\n",
            "Start phrase token index: 2\n",
            "End phrase token index: 3\n"
          ]
        }
      ],
      "source": [
        "idx = 100\n",
        "print(f\"Index: {idx}\")\n",
        "print(\"======= Encoder =======\")\n",
        "print(f\"Input phrase: {input_phrases[idx]}\")\n",
        "print(f\"Vector: {X_encoder[idx]}\")\n",
        "print(f\"Max. length: {encoder_vec.max_length}\")\n",
        "print(\"======= Decoder =======\")\n",
        "print(f\"Input phrase: {output_phrases[idx]}\")\n",
        "print(f\"Vector: {X_decoder[idx]}\")\n",
        "print(f\"Output phrase: {output_phrases[idx]}\")\n",
        "print(f\"Vector: {Y_decoder[idx]}\")\n",
        "print(f\"Max. length: {decoder_vec.max_length}\")\n",
        "print(\"==============\")\n",
        "print(f\"Start phrase token index: {decoder_vec.vocabulary.index(START_TOKEN)}\")\n",
        "print(f\"End phrase token index: {decoder_vec.vocabulary.index(END_TOKEN)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Qm_uDxJonO"
      },
      "source": [
        "# 2. Construct Encoder-Decoder NN with Bahdanau attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vPXQD5tFjzwp"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  '''\n",
        "    Encoder for using with Bahdanau attention\n",
        "  '''\n",
        "  def __init__(self, input_vocab: int, embedding_dim: int, lstm_hidden_units: int):\n",
        "    '''\n",
        "      :param input_vocab - vocabluary dimension of the source language\n",
        "      :param embedding_dim - dimension of the source language words embeddings\n",
        "      :param lstm_hidden_units - the number of the LSTM cell units\n",
        "    '''\n",
        "    super(Encoder, self).__init__()\n",
        "    self.lstm_hidden_units = lstm_hidden_units\n",
        "    self.emedding = Embedding(input_dim=input_vocab,\n",
        "                              output_dim=embedding_dim,\n",
        "                              mask_zero=True,\n",
        "                              name='encoder_embedding')\n",
        "    self.lstm = LSTM(units=lstm_hidden_units,\n",
        "                     return_sequences=True,\n",
        "                     return_state=True,\n",
        "                     name='encoder_lstm')\n",
        "\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "      Calculate forward propagation through the Encoder\n",
        "      :param x - input sequence (batch_size, sequence_length)\n",
        "    '''\n",
        "    # Get embeddings.\n",
        "    # 'x' dimension is (batch_size, sequence_length)\n",
        "    # 'out' dimension is (batch_size, sequence_length, embedding_dim)\n",
        "    out = self.emedding(x)\n",
        "\n",
        "    # Hence we don't need LSTM output, we get only LSTM states (hidden state and cell state)\n",
        "    # One of the problems here is that dispite the return_sequences parameter is True,\n",
        "    # we get only last (after propagation a whole sequence) values of the states and\n",
        "    # didn't get the states after each timestep. We will struggle with this later.\n",
        "    _, h, c = self.lstm(out)\n",
        "    return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B4DhtzjnPE_5"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Layers implements a Bahdanau attention mechanism\n",
        "  '''\n",
        "  def __init__(self, units: int, name=None):\n",
        "    '''\n",
        "      :param units - the number of the encoder and decoder hidden units.\n",
        "      This value can be obtained from the inputs dimensions but for the purpose of simplicity we will set it here.\n",
        "      :param name - tne name of the layer\n",
        "    '''\n",
        "    super(BahdanauAttention, self).__init__(name=name)\n",
        "    self.units = units\n",
        "    self.fc_encoder_states = Dense(units=units, activation='linear')\n",
        "    self.fc_decoder_states = Dense(units=units, activation='linear')\n",
        "    self.fc_combined = Dense(units=1, activation='linear')\n",
        "\n",
        "  def __call__(self, encoder_states, decoder_hidden_state):\n",
        "    '''\n",
        "      Calculate forward propagation through the Layer\n",
        "      :param encoder_states - encoder hidden states (batch_size, sequence_length, encoder_lstm_hidden_units)\n",
        "      :param decoder_hidden_state - decoder hidden state (batch_size, decoder_lstm_hidden_units)\n",
        "    '''\n",
        "\n",
        "    # Linear layer for the encoder hidden states (it has its own trainable weights).\n",
        "    fc_encoder_out = self.fc_encoder_states(encoder_states) # fc_encoder_out dimension is (batch_size, sequence_length, encoder_lstm_hidden_units)\n",
        "\n",
        "    # Linear layer for the decoder hidden state from the previous timestep (it has its own trainable weights).\n",
        "    fc_decoder_out = self.fc_decoder_states(decoder_hidden_state) # fc_decoder_out dimension is (batch_size, decoder_lstm_hidden_units)\n",
        "\n",
        "    # Add additional dimension to the fc_decoder_out\n",
        "    fc_decoder_out = tf.expand_dims(fc_decoder_out, axis=1) # fc_decoder_out dimension is (batch_size, 1, decoder_lstm_hidden_units)\n",
        "\n",
        "    # Calculate alignment score using linear layer  (it has its own trainable weights).\n",
        "    # Alignment_scores dimension is (batch_size, sequence_length)\n",
        "    alignment_scores = self.fc_combined(tf.math.tanh(fc_encoder_out + fc_decoder_out))\n",
        "\n",
        "    # Calculate attention weights of the each encoder hidden state within a batch\n",
        "    # softmax_alignment_scores dimension is (batch_size, sequence_length)\n",
        "    softmax_alignment_scores = tf.nn.softmax(alignment_scores)\n",
        "\n",
        "    # Calculate context vector. Its dimension is (batch_size, encoder_lstm_hidden_units)\n",
        "    context_vector = tf.reduce_sum(softmax_alignment_scores * encoder_states, axis=1)\n",
        "    return context_vector, softmax_alignment_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tL1KwedDnTc3"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  '''\n",
        "    Decoder with Bahdanau attention mechanism\n",
        "  '''\n",
        "  def __init__(self, output_vocab, embedding_dim, lstm_hidden_units):\n",
        "    '''\n",
        "      :param output_vocab - vocabluary dimension of the target language\n",
        "      :param embedding_dim - dimension of the target language words embeddings\n",
        "      :param lstm_hidden_units - the number of the LSTM cell units\n",
        "    '''\n",
        "    super(Decoder, self).__init__()\n",
        "    self.emedding = Embedding(input_dim=output_vocab,\n",
        "                              output_dim=embedding_dim,\n",
        "                              mask_zero=True,\n",
        "                              name='decoder_embedding')\n",
        "\n",
        "    self.lstm = LSTM(units=lstm_hidden_units,\n",
        "                     return_sequences=True,\n",
        "                     return_state=True,\n",
        "                     name='decoder_lstm')\n",
        "\n",
        "    self.attention = BahdanauAttention(units=lstm_hidden_units,\n",
        "                                       name='decoder_attention')\n",
        "\n",
        "    # Dense layer with softmax activation function\n",
        "    self.output_dense = Dense(units=output_vocab,\n",
        "                              activation='softmax',\n",
        "                              name='decoder_output')\n",
        "\n",
        "  def __call__(self, x, decoder_states, encoder_states):\n",
        "    '''\n",
        "      Calculate forward propagation through the Decoder\n",
        "      : param x - input sequence (batch_size, sequence_length_of_target_lang)\n",
        "      :param decoder_states - hidden and cell decoder states from the previous timestep (or last encoder states for the first timestep)\n",
        "      Dimension ((batch_size, lstm_hidden_units), (batch_size, lstm_hidden_units))\n",
        "      :param encoder_states - hidden encoder states from the each timesteps  (batch_size, sequence_length, lstm_hidden_units)\n",
        "    '''\n",
        "    # Unpack decoder states\n",
        "    hidden_state, cell_state = decoder_states\n",
        "\n",
        "    # Calculate contect vector based on Bahdanau attention mechanism\n",
        "    # context_vector dimension is (batch_size, lstm_hidden_units)\n",
        "    context_vector, _ = self.attention(encoder_states, hidden_state)\n",
        "\n",
        "    # Get target language embedding\n",
        "    out = self.emedding(x)\n",
        "\n",
        "    # Concatenate context_vector with the next embedded token\n",
        "    input = tf.expand_dims(tf.concat([context_vector, out], axis=-1), 1)\n",
        "\n",
        "    # Get LSTM outputs\n",
        "    out, h, c = self.lstm(input, initial_state=decoder_states)\n",
        "\n",
        "    # Propagate LSTM output through dense layer with softmax activation function\n",
        "    out = self.output_dense(out)\n",
        "    return out, h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "caH2BctcgtQ_"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqBahdanauAttention(tf.keras.Model):\n",
        "  '''\n",
        "  Encoder-Decoder network implements the Bahdanau attention mechanism\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               input_vocab,\n",
        "               output_vocab,\n",
        "               encoder_embd_dim,\n",
        "               decoder_embd_dim,\n",
        "               encoder_lstm_units,\n",
        "               decoder_lstm_units,\n",
        "               max_output_length,\n",
        "               start_token_index,\n",
        "               end_token_index):\n",
        "    '''\n",
        "      :param input_vocab - vocabluary dimension of the source language\n",
        "      :param output_vocab - vocabluary dimension of the target language\n",
        "      :param encoder_embd_dim - dimension of the source language words embeddings\n",
        "      :param decoder_embd_dim - dimension of the target language words embeddings\n",
        "      :param encoder_lstm_units - the number of the LSTM cell units\n",
        "      :param decoder_lstm_units - the number of the LSTM cell units\n",
        "      :param max_output_length - the maximum length of the output sequence\n",
        "      :param start_token_index - index of the start token in the output vocabulary\n",
        "      :param end_token_index - index of the end token in the output vocabulary\n",
        "\n",
        "    '''\n",
        "    super(Seq2SeqBahdanauAttention, self).__init__()\n",
        "    self.encoder = Encoder(input_vocab=input_vocab,\n",
        "                           embedding_dim=encoder_embd_dim,\n",
        "                           lstm_hidden_units=encoder_lstm_units)\n",
        "    self.decoder = Decoder(output_vocab=output_vocab,\n",
        "                           embedding_dim=decoder_embd_dim,\n",
        "                           lstm_hidden_units=decoder_lstm_units)\n",
        "    self.max_output_length = max_output_length\n",
        "    self.start_token_index = start_token_index\n",
        "    self.end_token_index = end_token_index\n",
        "    self.__loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
        "\n",
        "\n",
        "  def __loss_function(self, true, pred):\n",
        "    '''\n",
        "    '''\n",
        "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
        "    loss_ = self.__loss_object(true, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "  def __forward(self, X_encoder, X_decoder=None, Y_decoder=None):\n",
        "    '''\n",
        "      Forward propagation\n",
        "      :param X_encoder - input encoder sequence (source language) (batch_size, sequence_length_in_the_source_lang)\n",
        "      :param X_decoder - input decoder sequence (target language) (batch_size, sequence_length_in_the_target_lang + 2)\n",
        "      (ex. [start_token_index, word1_index, woprd2_index, end_token_index)\n",
        "      :param Y_decoder - output decoder sequence (target language) (batch_size, sequence_length_in_the_target_lang + 1)\n",
        "      (ex. [word1_index, woprd2_index, end_token_index)\n",
        "\n",
        "      We use X_decoder=None and Y_decoder=None for the purpose of prediction\n",
        "      We use X_decoder=None and Y_decoder is not None for the purpose of validation during training\n",
        "    '''\n",
        "    output = []\n",
        "    batch_size = X_encoder.shape[0]\n",
        "    encoder_states = []\n",
        "    loss = 0\n",
        "    accuracy = np.array([])\n",
        "\n",
        "    # Here we deal with the mentioned earlier problem of getting encoder hidden states on each timestep.\n",
        "    # As we mentoined before LSTM parameter 'return_sequences' doesn't affect on the hidden and cell states, i.e.\n",
        "    # we can't get LSTM states after every timestep. Therefore we need do it manually. It means the following:\n",
        "    # 1) we take the first token of the input sequence, propagate it through the encoder and save the LSTM states.\n",
        "    # 2) we take the two first tokens of the input sequence, propagate them through the encoder and save the last LSTM states.\n",
        "    # 3) We repeat step 2 adding the next token and save the last LSTM states.\n",
        "\n",
        "    for t in range(X_encoder.shape[1]):\n",
        "      h, c = self.encoder(X_encoder[:, :t+1])\n",
        "      encoder_states.append(h)\n",
        "    encoder_states = tf.stack(encoder_states, axis=1) # make tensor from the list\n",
        "\n",
        "    # save the last encoder hidden and cell states, since they are the initial decoder states\n",
        "    hidden_state = encoder_states[:, -1, :]\n",
        "    cell_state = c\n",
        "\n",
        "    if X_decoder is not None and Y_decoder is not None: # if we train network\n",
        "      # for every timestep (i.e. every token) of the target language sequence\n",
        "      for t in range(X_decoder.shape[1]):\n",
        "        # Set the decoder_input to the t-th token of the decoder input sequence.\n",
        "        # We use here the teacher forcing method for faster and efficient decoder training.\n",
        "        # The method uses the ground true as the decoder input instead of the prediction\n",
        "        # from the previous timestep.\n",
        "        decoder_input = X_decoder[:, t]\n",
        "        # print(decoder_input.shape)\n",
        "\n",
        "        # Calculate decoder output and states. We\n",
        "        out, hidden_state, cell_state = self.decoder(x=decoder_input,\n",
        "                                                     decoder_states=(hidden_state, cell_state),\n",
        "                                                     encoder_states=encoder_states)\n",
        "        # Collect output token\n",
        "        output.append(out)\n",
        "        loss += self.__loss_function(Y_decoder[:, t], out) # Calculate loss function\n",
        "        # Calculate accuracy\n",
        "        accuracy = np.hstack((accuracy, tf.keras.metrics.sparse_categorical_accuracy(Y_decoder[:, t], np.squeeze(out))))\n",
        "    else: # if we validate (calculate loss and accuracy on the test set) the network or make prediction\n",
        "        current_step = 0 # current timestep\n",
        "\n",
        "        # Set the first decoder input to the start token index. decoder_input dimension is (batch_size, )\n",
        "        decoder_input = np.full((batch_size, ), self.start_token_index)\n",
        "\n",
        "        while current_step<self.max_output_length:\n",
        "          out, hidden_state, cell_state = self.decoder(x=decoder_input,\n",
        "                                                       decoder_states=(hidden_state, cell_state),\n",
        "                                                       encoder_states=encoder_states)\n",
        "          if Y_decoder is not None: # if we validate the network\n",
        "            true, pred = Y_decoder[:, current_step], np.squeeze(out)\n",
        "            loss += self.__loss_function(true, pred)\n",
        "            accuracy = np.hstack((accuracy, tf.keras.metrics.sparse_categorical_accuracy(true, pred)))\n",
        "\n",
        "          tokens = np.argmax(np.squeeze(out), axis=1)\n",
        "          decoder_input = tokens\n",
        "          output.append(out)\n",
        "          current_step += 1\n",
        "\n",
        "    batch_loss = tf.reduce_sum(loss) / Y_decoder.shape[0] if Y_decoder is not None else None\n",
        "    return output, loss, batch_loss, accuracy\n",
        "\n",
        "  def __train_step(self, X_encoder, X_decoder, Y_decoder, learning_rate):\n",
        "    '''\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    with tf.GradientTape() as tape:\n",
        "      _, loss, batch_loss, accuracy = self.__forward(X_encoder, X_decoder, Y_decoder)\n",
        "      variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "      gradients = tape.gradient(loss, variables)\n",
        "      optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss, accuracy\n",
        "\n",
        "  def __align_to_length(self, X, batch_size=64):\n",
        "    '''\n",
        "    '''\n",
        "    c = Counter([len(x[0]) for x in X])\n",
        "    X_new = []\n",
        "    for length, count in c.most_common():\n",
        "      if count >= batch_size:\n",
        "        batches_count = count//batch_size\n",
        "        X_new += list(filter(lambda x: len(x[0])==length, X))[:batches_count*batch_size]\n",
        "    return X_new\n",
        "\n",
        "  def __generate_batches(self, X, Y, batch_size=64):\n",
        "    '''\n",
        "    '''\n",
        "    X_encoder, X_decoder = zip(*X)\n",
        "    ds = list(zip(X_encoder, X_decoder, Y))\n",
        "    X_new = self.__align_to_length(ds, batch_size)\n",
        "    X_encoder, X_decoder, Y_decoder = zip(*X_new)\n",
        "    X_encoder, X_decoder, Y_decoder = list(X_encoder), list(X_decoder), list(Y_decoder)\n",
        "    batches_count = len(X_encoder) // batch_size\n",
        "    for i in range(batches_count):\n",
        "      lower_idx, upper_idx = i*batch_size, (i+1)*batch_size\n",
        "      yield np.array(X_encoder[lower_idx:upper_idx]), \\\n",
        "            np.array(X_decoder[lower_idx:upper_idx]), \\\n",
        "            np.array(Y_decoder[lower_idx:upper_idx])\n",
        "\n",
        "  def fit(self, X_encoder, X_decoder, Y_decoder, epoch=20, batch_size=64, train_size=0.8, learning_rate=0.001):\n",
        "    '''\n",
        "      :param learning_rate - optimizer learning rate\n",
        "    '''\n",
        "    X_train, X_test, Y_decoder_train, Y_decoder_test = train_test_split(list(zip(X_encoder, X_decoder)),\n",
        "                                                                        Y_decoder,\n",
        "                                                                        train_size=train_size)\n",
        "    # X_encoder_train, X_decoder_train = zip(*X_train)\n",
        "    # X = list(zip(X_encoder_train, X_decoder_train, Y_decoder_train))\n",
        "    # X_new = self.align_to_length(X, batch_size)\n",
        "    # X_encoder_train, X_decoder_train, Y_decoder_train = zip(*X_new)\n",
        "\n",
        "    # train_ds = tf.data.Dataset.from_tensor_slices((X_encoder_train, X_decoder_train, Y_decoder_train)).batch(batch_size)\n",
        "    # train_ds_size = train_ds.cardinality()\n",
        "\n",
        "    # X_encoder_test, X_decoder_test = zip(*X_test)\n",
        "    # test_ds = tf.data.Dataset.from_tensor_slices((X_encoder_test, X_decoder_test, Y_decoder_test))\n",
        "\n",
        "    # M = len(X_encoder)\n",
        "    # train_sample_size = int(train_size*M)\n",
        "    # full_dataset = tf.data.Dataset.from_tensor_slices((X_encoder, X_decoder, Y_decoder))\n",
        "    # train_ds = full_dataset.take(train_sample_size)\n",
        "    # train_ds_size = train_ds.cardinality()\n",
        "    # train_ds = train_ds.batch(batch_size)\n",
        "    # test_ds = full_dataset.skip(train_sample_size)\n",
        "\n",
        "    X_new_train = self.__align_to_length(X_train, batch_size)\n",
        "    X_new_test = self.__align_to_length(X_test, batch_size)\n",
        "    total_train_batches = len(X_new_train) // batch_size\n",
        "    total_test_batches = len(X_new_test) // batch_size\n",
        "    train_ds_size = len(X_new_train)\n",
        "    test_ds_size = len(X_new_test)\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_accuracy\": [], \"test_loss\": [], \"test_accuracy\": []}\n",
        "\n",
        "    print(f\"Train dataset: {total_train_batches} batches, {train_ds_size} samples\")\n",
        "    print(f\"Test dataset: {total_test_batches} batches, {test_ds_size} samples\")\n",
        "    print(f\"{'='*10}\")\n",
        "    for ep in range(1, epoch+1):\n",
        "      print(f\"Epoch {ep}/{epoch}\")\n",
        "      total_loss = 0\n",
        "      accuracy = np.array([])\n",
        "      for batch, (X_batch_encoder,\n",
        "                  X_batch_decoder,\n",
        "                  Y_batch_decoder) in tqdm(enumerate(self.__generate_batches(X_train,\n",
        "                                                                             Y_decoder_train,\n",
        "                                                                             batch_size=batch_size)),\n",
        "                                           desc=f\"Train dataset\"):\n",
        "        batch_loss, batch_accuracy = self.__train_step(X_batch_encoder, X_batch_decoder, Y_batch_decoder, learning_rate)\n",
        "\n",
        "        total_loss += batch_loss\n",
        "        accuracy = np.hstack((accuracy, batch_accuracy))\n",
        "\n",
        "        # if batch%1000==0:\n",
        "        #   print(f\"Loss: {total_loss.numpy()/batch_size}. Accuracy: {np.mean(accuracy)}\")\n",
        "\n",
        "      total_loss /= batch_size\n",
        "      history[\"train_loss\"].append(total_loss.numpy())\n",
        "      history[\"train_accuracy\"].append(np.mean(accuracy))\n",
        "      print(f\"Loss on train: {total_loss.numpy():.4f} Accuracy on train: {np.mean(accuracy):.4f}\")\n",
        "\n",
        "      total_loss = 0\n",
        "      accuracy = np.array([])\n",
        "      for batch, (X_batch_encoder,\n",
        "                  _,\n",
        "                  Y_batch_decoder) in tqdm(enumerate(self.__generate_batches(X_test,\n",
        "                                                                             Y_decoder_test,\n",
        "                                                                             batch_size=batch_size)),\n",
        "                                           desc=f\"Test dataset\"):\n",
        "        _, _, batch_loss, batch_accuracy = self.__forward(X_batch_encoder,\n",
        "                                                          None,\n",
        "                                                          Y_batch_decoder)\n",
        "        total_loss += batch_loss\n",
        "        accuracy = np.hstack((accuracy, batch_accuracy))\n",
        "\n",
        "      total_loss /= batch_size\n",
        "      history[\"test_loss\"].append(total_loss.numpy())\n",
        "      history[\"test_accuracy\"].append(np.mean(accuracy))\n",
        "      print(f\"Loss on test: {total_loss.numpy():.4f} Accuracy on test: {np.mean(accuracy):.4f}\")\n",
        "    return history\n",
        "\n",
        "  def predict(self, X_encoder):\n",
        "    '''\n",
        "    '''\n",
        "    out, _, _, _ = self.__forward(X_encoder)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "R7HQ2-Sqlpye"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 64\n",
        "lstm_hidden_units = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Jn12K0-qgtY0"
      },
      "outputs": [],
      "source": [
        "model = Seq2SeqBahdanauAttention(input_vocab=input_vocab,\n",
        "                                 output_vocab=output_vocab,\n",
        "                                 encoder_embd_dim=embedding_dim,\n",
        "                                 decoder_embd_dim=embedding_dim,\n",
        "                                 encoder_lstm_units=lstm_hidden_units,\n",
        "                                 decoder_lstm_units=lstm_hidden_units,\n",
        "                                 max_output_length=decoder_vec.max_length,\n",
        "                                 start_token_index=decoder_vec.vocabulary.index(START_TOKEN),\n",
        "                                 end_token_index=decoder_vec.vocabulary.index(END_TOKEN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RCJ4QCjxGVnB",
        "outputId": "37ce5dcf-e1ca-459b-9d73-e4069d8c0c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 23 batches, 736 samples\n",
            "Test dataset: 5 batches, 160 samples\n",
            "==========\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:13,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.9193 Accuracy on train: 0.1940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:02,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.0905\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:12,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.7189 Accuracy on train: 0.3978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:02,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.1974\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:13,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.6282 Accuracy on train: 0.4701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:01,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.1750\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:14,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.5799 Accuracy on train: 0.5038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 2it [00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:01,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2414\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:14,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.5314 Accuracy on train: 0.5447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:00,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:01,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2284\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:14,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.4903 Accuracy on train: 0.5759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:00,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:01,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2603\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:14,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.4554 Accuracy on train: 0.6030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  3.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:00,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:01,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2569\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:13,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.4198 Accuracy on train: 0.6327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:01,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:02,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2853\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:14,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.3866 Accuracy on train: 0.6587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:01,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:02,  1.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2716\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:15,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.3619 Accuracy on train: 0.6807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:01,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:02,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2957\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:15,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.3348 Accuracy on train: 0.7040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:01,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test dataset: 5it [00:02,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n",
            "Loss on test: nan Accuracy on test: 0.2759\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:13,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.3121 Accuracy on train: 0.7283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 2it [00:00,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(14,) (14, 2000)\n",
            "(1,) (1, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 3it [00:01,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(10,) (10, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 4it [00:01,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 5it [00:02,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(29,) (29, 2000)\n",
            "(19,) (19, 2000)\n",
            "(4,) (4, 2000)\n",
            "(0,) (0, 2000)\n",
            "(0,) (0, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 5it [00:02,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on test: nan Accuracy on test: 0.3052\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train dataset: 23it [00:13,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on train: 0.2946 Accuracy on train: 0.7375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(28,) (28, 2000)\n",
            "(17,) (17, 2000)\n",
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(32,) (32, 2000)\n",
            "(17,) (17, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest dataset: 1it [00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2,) (2, 2000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-bfe2b6ee1911>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-c5ce206d07a9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_encoder, X_decoder, Y_decoder, epoch, batch_size, train_size, learning_rate)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                                                              batch_size=batch_size)),\n\u001b[1;32m    233\u001b[0m                                            desc=f\"Test dataset\"):\n\u001b[0;32m--> 234\u001b[0;31m         _, _, batch_loss, batch_accuracy = self.__forward(X_batch_encoder,\n\u001b[0m\u001b[1;32m    235\u001b[0m                                                           \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                                                           Y_batch_decoder)\n",
            "\u001b[0;32m<ipython-input-58-c5ce206d07a9>\u001b[0m in \u001b[0;36m__forward\u001b[0;34m(self, X_encoder, X_decoder, Y_decoder)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-c5ce206d07a9>\u001b[0m in \u001b[0;36m__loss_function\u001b[0;34m(self, true, pred)\u001b[0m\n\u001b[1;32m     42\u001b[0m     '''\n\u001b[1;32m     43\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mloss_\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 )\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0min_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         )\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2076\u001b[0m       \u001b[0mSparse\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mcrossentropy\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m     \"\"\"\n\u001b[0;32m-> 2078\u001b[0;31m     return backend.sparse_categorical_crossentropy(\n\u001b[0m\u001b[1;32m   2079\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   5615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5616\u001b[0m         \u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5617\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5618\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m   \u001b[0;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  11340\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11341\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11342\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  11343\u001b[0m         _ctx, \"Sub\", name, x, y)\n\u001b[1;32m  11344\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = model.fit(X_encoder, X_decoder, Y_decoder, epoch=20, batch_size=32, train_size=0.8, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_weights(base_path + 'Data/machine_translation_bahdanau_attention_weights.h5')\n",
        "with open(base_path + 'Data/machine_translation_bahdanau_attention_history.pickle', 'wb') as f:\n",
        "    pickle.dump(history, f)"
      ],
      "metadata": {
        "id": "e1mQ2TQbWItn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(base_path + 'Data/machine_translation_bahdanau_attention_history.pickle', 'rb') as f:\n",
        "#     history = pickle.load(f)"
      ],
      "metadata": {
        "id": "W_TqkLyyDfgF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=history)\n",
        "# Plot the learning curves (the loss function and the accuracy metric)\n",
        "# which calculated on the training and validation datasets\n",
        "_, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "sns.lineplot(data=df[['train_loss', 'test_loss']], ax=axs[0])\n",
        "sns.lineplot(data=df[['train_accuracy', 'test_accuracy']], ax=axs[1])\n",
        "axs[0].set_title('Loss function')\n",
        "axs[1].set_title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BLfI5X5UF8rX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNBdqFh0fKQ7PnSDwxtcw4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}